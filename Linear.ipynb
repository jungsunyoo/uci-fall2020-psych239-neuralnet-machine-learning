{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoojungsun0/Psych239/blob/master/Linear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTqWQ1GTSqeY",
        "outputId": "c8149951-5bd3-4fc6-fc07-f4c1283fb979"
      },
      "source": [
        "!pip install thop"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: thop in /usr/local/lib/python3.6/dist-packages (0.0.31.post2005241907)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from thop) (1.7.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2jZOoMMQ2NA",
        "outputId": "42cfe411-4594-43c6-f562-f79436f49f0c"
      },
      "source": [
        "\n",
        "\n",
        "import argparse\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from thop import profile, clever_format\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "import sys\n",
        "%cd /content/gdrive/MyDrive/'Colab Notebooks'/SimCLR\n",
        "\n",
        "\n",
        "import utils\n",
        "from model import Model"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "/content/gdrive/MyDrive/Colab Notebooks/SimCLR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXpV3lvBQ75r"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_class, pretrained_path):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # encoder\n",
        "        self.f = Model().f\n",
        "        # classifier\n",
        "        self.fc = nn.Linear(2048, num_class, bias=True)\n",
        "        self.load_state_dict(torch.load(pretrained_path, map_location='cpu'), strict=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(x)\n",
        "        feature = torch.flatten(x, start_dim=1)\n",
        "        out = self.fc(feature)\n",
        "        return out\n",
        "\n",
        "\n",
        "# train or test for one epoch\n",
        "def train_val(net, data_loader, train_optimizer):\n",
        "    is_train = train_optimizer is not None\n",
        "    net.train() if is_train else net.eval()\n",
        "\n",
        "    total_loss, total_correct_1, total_correct_5, total_num, data_bar = 0.0, 0.0, 0.0, 0, tqdm(data_loader)\n",
        "    with (torch.enable_grad() if is_train else torch.no_grad()):\n",
        "        for data, target in data_bar:\n",
        "            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
        "            out = net(data)\n",
        "            loss = loss_criterion(out, target)\n",
        "\n",
        "            if is_train:\n",
        "                train_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                train_optimizer.step()\n",
        "\n",
        "            total_num += data.size(0)\n",
        "            total_loss += loss.item() * data.size(0)\n",
        "            prediction = torch.argsort(out, dim=-1, descending=True)\n",
        "            total_correct_1 += torch.sum((prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
        "            total_correct_5 += torch.sum((prediction[:, 0:5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
        "\n",
        "            data_bar.set_description('{} Epoch: [{}/{}] Loss: {:.4f} ACC@1: {:.2f}% ACC@5: {:.2f}%'\n",
        "                                     .format('Train' if is_train else 'Test', epoch, epochs, total_loss / total_num,\n",
        "                                             total_correct_1 / total_num * 100, total_correct_5 / total_num * 100))\n",
        "\n",
        "    return total_loss / total_num, total_correct_1 / total_num * 100, total_correct_5 / total_num * 100"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2WPPuCZQ8EC",
        "outputId": "8f2e8e1d-8baf-4751-840a-b9f4eb935e1c"
      },
      "source": [
        "# import os\n",
        "# rootdir = '../'\n",
        "model_path = 'results/Model1_baseline/128_0.5_200_256_100_model.pth'\n",
        "# model_path = os.path.join(rootdir, path)\n",
        "#'results/128_0.5_200_256_100_model.pth'\n",
        "batch_size = 256 #512\n",
        "epochs = 50 #100\n",
        "\n",
        "train_data = CIFAR10(root='data', train=True, transform=utils.train_transform, download=True)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
        "test_data = CIFAR10(root='data', train=False, transform=utils.test_transform, download=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
        "\n",
        "model = Net(num_class=len(train_data.classes), pretrained_path=model_path).cuda()\n",
        "for param in model.f.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "flops, params = profile(model, inputs=(torch.randn(1, 3, 32, 32).cuda(),))\n",
        "flops, params = clever_format([flops, params])\n",
        "print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3, weight_decay=1e-6)\n",
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "results = {'train_loss': [], 'train_acc@1': [], 'train_acc@5': [],\n",
        "            'test_loss': [], 'test_acc@1': [], 'test_acc@5': []}\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss, train_acc_1, train_acc_5 = train_val(model, train_loader, optimizer)\n",
        "    results['train_loss'].append(train_loss)\n",
        "    results['train_acc@1'].append(train_acc_1)\n",
        "    results['train_acc@5'].append(train_acc_5)\n",
        "    test_loss, test_acc_1, test_acc_5 = train_val(model, test_loader, None)\n",
        "    results['test_loss'].append(test_loss)\n",
        "    results['test_acc@1'].append(test_acc_1)\n",
        "    results['test_acc@5'].append(test_acc_5)\n",
        "    # save statistics\n",
        "    data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))\n",
        "    data_frame.to_csv('results/Model1_baseline/linear_statistics.csv', index_label='epoch')\n",
        "    if test_acc_1 > best_acc:\n",
        "        best_acc = test_acc_1\n",
        "        torch.save(model.state_dict(), 'results/Model1_baseline/linear_model.pth')\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/196 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torchvision.models.resnet.Bottleneck'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.Net'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "# Model Params: 23.52M FLOPs: 1.30G\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: [1/50] Loss: 0.8773 ACC@1: 71.54% ACC@5: 96.93%: 100%|██████████| 196/196 [00:52<00:00,  3.73it/s]\n",
            "Test Epoch: [1/50] Loss: 0.5911 ACC@1: 79.91% ACC@5: 98.95%: 100%|██████████| 40/40 [00:05<00:00,  7.34it/s]\n",
            "Train Epoch: [2/50] Loss: 0.7168 ACC@1: 74.97% ACC@5: 97.97%: 100%|██████████| 196/196 [00:52<00:00,  3.70it/s]\n",
            "Test Epoch: [2/50] Loss: 0.5495 ACC@1: 81.02% ACC@5: 99.22%: 100%|██████████| 40/40 [00:05<00:00,  7.24it/s]\n",
            "Train Epoch: [3/50] Loss: 0.6965 ACC@1: 75.69% ACC@5: 98.09%: 100%|██████████| 196/196 [00:52<00:00,  3.76it/s]\n",
            "Test Epoch: [3/50] Loss: 0.5277 ACC@1: 81.57% ACC@5: 99.38%: 100%|██████████| 40/40 [00:05<00:00,  7.49it/s]\n",
            "Train Epoch: [4/50] Loss: 0.6814 ACC@1: 76.34% ACC@5: 98.17%: 100%|██████████| 196/196 [00:51<00:00,  3.79it/s]\n",
            "Test Epoch: [4/50] Loss: 0.5163 ACC@1: 82.08% ACC@5: 99.35%: 100%|██████████| 40/40 [00:05<00:00,  7.35it/s]\n",
            "Train Epoch: [5/50] Loss: 0.6777 ACC@1: 76.56% ACC@5: 98.09%: 100%|██████████| 196/196 [00:52<00:00,  3.75it/s]\n",
            "Test Epoch: [5/50] Loss: 0.5049 ACC@1: 82.46% ACC@5: 99.47%: 100%|██████████| 40/40 [00:05<00:00,  7.43it/s]\n",
            "Train Epoch: [6/50] Loss: 0.6720 ACC@1: 76.40% ACC@5: 98.15%: 100%|██████████| 196/196 [00:52<00:00,  3.76it/s]\n",
            "Test Epoch: [6/50] Loss: 0.4960 ACC@1: 82.98% ACC@5: 99.46%: 100%|██████████| 40/40 [00:05<00:00,  7.56it/s]\n",
            "Train Epoch: [7/50] Loss: 0.6691 ACC@1: 76.68% ACC@5: 98.06%: 100%|██████████| 196/196 [00:51<00:00,  3.77it/s]\n",
            "Test Epoch: [7/50] Loss: 0.4900 ACC@1: 83.01% ACC@5: 99.46%: 100%|██████████| 40/40 [00:05<00:00,  7.47it/s]\n",
            "Train Epoch: [8/50] Loss: 0.6655 ACC@1: 76.87% ACC@5: 98.15%: 100%|██████████| 196/196 [00:51<00:00,  3.80it/s]\n",
            "Test Epoch: [8/50] Loss: 0.4908 ACC@1: 83.00% ACC@5: 99.41%: 100%|██████████| 40/40 [00:05<00:00,  7.44it/s]\n",
            "Train Epoch: [9/50] Loss: 0.6526 ACC@1: 77.51% ACC@5: 98.17%: 100%|██████████| 196/196 [00:51<00:00,  3.78it/s]\n",
            "Test Epoch: [9/50] Loss: 0.4845 ACC@1: 83.47% ACC@5: 99.47%: 100%|██████████| 40/40 [00:05<00:00,  7.43it/s]\n",
            "Train Epoch: [10/50] Loss: 0.6487 ACC@1: 77.43% ACC@5: 98.28%: 100%|██████████| 196/196 [00:53<00:00,  3.69it/s]\n",
            "Test Epoch: [10/50] Loss: 0.4791 ACC@1: 83.24% ACC@5: 99.44%: 100%|██████████| 40/40 [00:05<00:00,  7.42it/s]\n",
            "Train Epoch: [11/50] Loss: 0.6501 ACC@1: 77.33% ACC@5: 98.24%: 100%|██████████| 196/196 [00:52<00:00,  3.70it/s]\n",
            "Test Epoch: [11/50] Loss: 0.4854 ACC@1: 83.02% ACC@5: 99.37%: 100%|██████████| 40/40 [00:05<00:00,  7.15it/s]\n",
            "Train Epoch: [12/50] Loss: 0.6504 ACC@1: 77.40% ACC@5: 98.25%: 100%|██████████| 196/196 [00:53<00:00,  3.70it/s]\n",
            "Test Epoch: [12/50] Loss: 0.4854 ACC@1: 83.27% ACC@5: 99.42%: 100%|██████████| 40/40 [00:05<00:00,  7.42it/s]\n",
            "Train Epoch: [13/50] Loss: 0.6412 ACC@1: 77.73% ACC@5: 98.24%: 100%|██████████| 196/196 [00:52<00:00,  3.73it/s]\n",
            "Test Epoch: [13/50] Loss: 0.4726 ACC@1: 83.85% ACC@5: 99.37%: 100%|██████████| 40/40 [00:05<00:00,  7.44it/s]\n",
            "Train Epoch: [14/50] Loss: 0.6426 ACC@1: 77.50% ACC@5: 98.22%: 100%|██████████| 196/196 [00:51<00:00,  3.83it/s]\n",
            "Test Epoch: [14/50] Loss: 0.4640 ACC@1: 84.01% ACC@5: 99.51%: 100%|██████████| 40/40 [00:05<00:00,  7.44it/s]\n",
            "Train Epoch: [15/50] Loss: 0.6468 ACC@1: 77.50% ACC@5: 98.27%: 100%|██████████| 196/196 [00:51<00:00,  3.84it/s]\n",
            "Test Epoch: [15/50] Loss: 0.4731 ACC@1: 83.62% ACC@5: 99.41%: 100%|██████████| 40/40 [00:05<00:00,  7.52it/s]\n",
            "Train Epoch: [16/50] Loss: 0.6435 ACC@1: 77.49% ACC@5: 98.27%: 100%|██████████| 196/196 [00:51<00:00,  3.82it/s]\n",
            "Test Epoch: [16/50] Loss: 0.4601 ACC@1: 83.98% ACC@5: 99.48%: 100%|██████████| 40/40 [00:05<00:00,  7.46it/s]\n",
            "Train Epoch: [17/50] Loss: 0.6342 ACC@1: 77.91% ACC@5: 98.38%: 100%|██████████| 196/196 [00:51<00:00,  3.80it/s]\n",
            "Test Epoch: [17/50] Loss: 0.4591 ACC@1: 84.18% ACC@5: 99.53%: 100%|██████████| 40/40 [00:05<00:00,  7.37it/s]\n",
            "Train Epoch: [18/50] Loss: 0.6420 ACC@1: 77.54% ACC@5: 98.24%: 100%|██████████| 196/196 [00:51<00:00,  3.79it/s]\n",
            "Test Epoch: [18/50] Loss: 0.4730 ACC@1: 83.82% ACC@5: 99.48%: 100%|██████████| 40/40 [00:05<00:00,  7.40it/s]\n",
            "Train Epoch: [19/50] Loss: 0.6372 ACC@1: 77.85% ACC@5: 98.25%: 100%|██████████| 196/196 [00:51<00:00,  3.79it/s]\n",
            "Test Epoch: [19/50] Loss: 0.4678 ACC@1: 83.90% ACC@5: 99.42%: 100%|██████████| 40/40 [00:05<00:00,  7.38it/s]\n",
            "Train Epoch: [20/50] Loss: 0.6333 ACC@1: 78.08% ACC@5: 98.29%: 100%|██████████| 196/196 [00:51<00:00,  3.79it/s]\n",
            "Test Epoch: [20/50] Loss: 0.4533 ACC@1: 84.48% ACC@5: 99.42%: 100%|██████████| 40/40 [00:05<00:00,  7.40it/s]\n",
            "Train Epoch: [21/50] Loss: 0.6320 ACC@1: 78.07% ACC@5: 98.28%: 100%|██████████| 196/196 [00:50<00:00,  3.85it/s]\n",
            "Test Epoch: [21/50] Loss: 0.4573 ACC@1: 84.35% ACC@5: 99.52%: 100%|██████████| 40/40 [00:05<00:00,  7.46it/s]\n",
            "Train Epoch: [22/50] Loss: 0.6318 ACC@1: 78.08% ACC@5: 98.37%: 100%|██████████| 196/196 [00:50<00:00,  3.87it/s]\n",
            "Test Epoch: [22/50] Loss: 0.4809 ACC@1: 83.51% ACC@5: 99.43%: 100%|██████████| 40/40 [00:05<00:00,  7.53it/s]\n",
            "Train Epoch: [23/50] Loss: 0.6307 ACC@1: 78.03% ACC@5: 98.34%: 100%|██████████| 196/196 [00:50<00:00,  3.84it/s]\n",
            "Test Epoch: [23/50] Loss: 0.4529 ACC@1: 84.38% ACC@5: 99.52%: 100%|██████████| 40/40 [00:05<00:00,  7.45it/s]\n",
            "Train Epoch: [24/50] Loss: 0.6274 ACC@1: 78.13% ACC@5: 98.36%: 100%|██████████| 196/196 [00:50<00:00,  3.89it/s]\n",
            "Test Epoch: [24/50] Loss: 0.4577 ACC@1: 84.06% ACC@5: 99.49%: 100%|██████████| 40/40 [00:05<00:00,  7.48it/s]\n",
            "Train Epoch: [25/50] Loss: 0.6313 ACC@1: 78.10% ACC@5: 98.34%: 100%|██████████| 196/196 [00:50<00:00,  3.88it/s]\n",
            "Test Epoch: [25/50] Loss: 0.4585 ACC@1: 84.55% ACC@5: 99.56%: 100%|██████████| 40/40 [00:05<00:00,  7.55it/s]\n",
            "Train Epoch: [26/50] Loss: 0.6242 ACC@1: 78.10% ACC@5: 98.35%: 100%|██████████| 196/196 [00:50<00:00,  3.88it/s]\n",
            "Test Epoch: [26/50] Loss: 0.4562 ACC@1: 84.09% ACC@5: 99.50%: 100%|██████████| 40/40 [00:05<00:00,  7.59it/s]\n",
            "Train Epoch: [27/50] Loss: 0.6259 ACC@1: 78.09% ACC@5: 98.30%: 100%|██████████| 196/196 [00:51<00:00,  3.78it/s]\n",
            "Test Epoch: [27/50] Loss: 0.4564 ACC@1: 84.31% ACC@5: 99.50%: 100%|██████████| 40/40 [00:05<00:00,  7.39it/s]\n",
            "Train Epoch: [28/50] Loss: 0.6307 ACC@1: 77.90% ACC@5: 98.22%: 100%|██████████| 196/196 [00:51<00:00,  3.83it/s]\n",
            "Test Epoch: [28/50] Loss: 0.4512 ACC@1: 84.72% ACC@5: 99.48%: 100%|██████████| 40/40 [00:05<00:00,  7.48it/s]\n",
            "Train Epoch: [29/50] Loss: 0.6240 ACC@1: 78.01% ACC@5: 98.38%: 100%|██████████| 196/196 [00:51<00:00,  3.83it/s]\n",
            "Test Epoch: [29/50] Loss: 0.4526 ACC@1: 84.59% ACC@5: 99.45%: 100%|██████████| 40/40 [00:05<00:00,  7.46it/s]\n",
            "Train Epoch: [30/50] Loss: 0.6230 ACC@1: 78.36% ACC@5: 98.33%: 100%|██████████| 196/196 [00:51<00:00,  3.84it/s]\n",
            "Test Epoch: [30/50] Loss: 0.4503 ACC@1: 84.33% ACC@5: 99.48%: 100%|██████████| 40/40 [00:05<00:00,  7.43it/s]\n",
            "Train Epoch: [31/50] Loss: 0.6238 ACC@1: 78.17% ACC@5: 98.33%: 100%|██████████| 196/196 [00:50<00:00,  3.90it/s]\n",
            "Test Epoch: [31/50] Loss: 0.4544 ACC@1: 84.32% ACC@5: 99.48%: 100%|██████████| 40/40 [00:05<00:00,  7.36it/s]\n",
            "Train Epoch: [32/50] Loss: 0.6272 ACC@1: 78.19% ACC@5: 98.30%: 100%|██████████| 196/196 [00:51<00:00,  3.82it/s]\n",
            "Test Epoch: [32/50] Loss: 0.4490 ACC@1: 84.63% ACC@5: 99.43%: 100%|██████████| 40/40 [00:05<00:00,  7.39it/s]\n",
            "Train Epoch: [33/50] Loss: 0.6141 ACC@1: 78.39% ACC@5: 98.41%: 100%|██████████| 196/196 [00:51<00:00,  3.79it/s]\n",
            "Test Epoch: [33/50] Loss: 0.4460 ACC@1: 84.79% ACC@5: 99.46%: 100%|██████████| 40/40 [00:05<00:00,  7.39it/s]\n",
            "Train Epoch: [34/50] Loss: 0.6203 ACC@1: 78.32% ACC@5: 98.37%: 100%|██████████| 196/196 [00:52<00:00,  3.73it/s]\n",
            "Test Epoch: [34/50] Loss: 0.4458 ACC@1: 84.35% ACC@5: 99.50%: 100%|██████████| 40/40 [00:05<00:00,  7.30it/s]\n",
            "Train Epoch: [35/50] Loss: 0.6203 ACC@1: 78.41% ACC@5: 98.34%: 100%|██████████| 196/196 [00:52<00:00,  3.76it/s]\n",
            "Test Epoch: [35/50] Loss: 0.4383 ACC@1: 84.72% ACC@5: 99.54%: 100%|██████████| 40/40 [00:05<00:00,  7.39it/s]\n",
            "Train Epoch: [36/50] Loss: 0.6167 ACC@1: 78.55% ACC@5: 98.27%: 100%|██████████| 196/196 [00:52<00:00,  3.71it/s]\n",
            "Test Epoch: [36/50] Loss: 0.4600 ACC@1: 84.07% ACC@5: 99.46%: 100%|██████████| 40/40 [00:05<00:00,  7.23it/s]\n",
            "Train Epoch: [37/50] Loss: 0.6190 ACC@1: 78.63% ACC@5: 98.34%: 100%|██████████| 196/196 [00:52<00:00,  3.71it/s]\n",
            "Test Epoch: [37/50] Loss: 0.4405 ACC@1: 85.00% ACC@5: 99.53%: 100%|██████████| 40/40 [00:05<00:00,  7.26it/s]\n",
            "Train Epoch: [38/50] Loss: 0.6156 ACC@1: 78.57% ACC@5: 98.31%: 100%|██████████| 196/196 [00:53<00:00,  3.65it/s]\n",
            "Test Epoch: [38/50] Loss: 0.4468 ACC@1: 84.88% ACC@5: 99.60%: 100%|██████████| 40/40 [00:05<00:00,  7.35it/s]\n",
            "Train Epoch: [39/50] Loss: 0.6110 ACC@1: 78.68% ACC@5: 98.40%: 100%|██████████| 196/196 [00:53<00:00,  3.69it/s]\n",
            "Test Epoch: [39/50] Loss: 0.4432 ACC@1: 84.66% ACC@5: 99.54%: 100%|██████████| 40/40 [00:05<00:00,  7.34it/s]\n",
            "Train Epoch: [40/50] Loss: 0.6156 ACC@1: 78.61% ACC@5: 98.33%: 100%|██████████| 196/196 [00:51<00:00,  3.77it/s]\n",
            "Test Epoch: [40/50] Loss: 0.4411 ACC@1: 85.00% ACC@5: 99.55%: 100%|██████████| 40/40 [00:05<00:00,  7.35it/s]\n",
            "Train Epoch: [41/50] Loss: 0.6147 ACC@1: 78.61% ACC@5: 98.36%: 100%|██████████| 196/196 [00:51<00:00,  3.78it/s]\n",
            "Test Epoch: [41/50] Loss: 0.4497 ACC@1: 84.43% ACC@5: 99.45%: 100%|██████████| 40/40 [00:05<00:00,  7.35it/s]\n",
            "Train Epoch: [42/50] Loss: 0.6164 ACC@1: 78.38% ACC@5: 98.32%: 100%|██████████| 196/196 [00:52<00:00,  3.73it/s]\n",
            "Test Epoch: [42/50] Loss: 0.4430 ACC@1: 84.88% ACC@5: 99.56%: 100%|██████████| 40/40 [00:05<00:00,  7.32it/s]\n",
            "Train Epoch: [43/50] Loss: 0.6133 ACC@1: 78.69% ACC@5: 98.35%: 100%|██████████| 196/196 [00:53<00:00,  3.64it/s]\n",
            "Test Epoch: [43/50] Loss: 0.4505 ACC@1: 84.51% ACC@5: 99.49%: 100%|██████████| 40/40 [00:05<00:00,  7.19it/s]\n",
            "Train Epoch: [44/50] Loss: 0.6087 ACC@1: 78.98% ACC@5: 98.33%: 100%|██████████| 196/196 [00:53<00:00,  3.66it/s]\n",
            "Test Epoch: [44/50] Loss: 0.4441 ACC@1: 84.56% ACC@5: 99.48%: 100%|██████████| 40/40 [00:05<00:00,  7.35it/s]\n",
            "Train Epoch: [45/50] Loss: 0.6098 ACC@1: 78.67% ACC@5: 98.39%: 100%|██████████| 196/196 [00:53<00:00,  3.67it/s]\n",
            "Test Epoch: [45/50] Loss: 0.4410 ACC@1: 84.78% ACC@5: 99.52%: 100%|██████████| 40/40 [00:05<00:00,  7.28it/s]\n",
            "Train Epoch: [46/50] Loss: 0.6115 ACC@1: 78.51% ACC@5: 98.40%: 100%|██████████| 196/196 [00:53<00:00,  3.65it/s]\n",
            "Test Epoch: [46/50] Loss: 0.4465 ACC@1: 84.44% ACC@5: 99.42%: 100%|██████████| 40/40 [00:05<00:00,  7.24it/s]\n",
            "Train Epoch: [47/50] Loss: 0.6132 ACC@1: 78.72% ACC@5: 98.31%: 100%|██████████| 196/196 [00:53<00:00,  3.65it/s]\n",
            "Test Epoch: [47/50] Loss: 0.4470 ACC@1: 84.65% ACC@5: 99.60%: 100%|██████████| 40/40 [00:05<00:00,  7.26it/s]\n",
            "Train Epoch: [48/50] Loss: 0.6147 ACC@1: 78.54% ACC@5: 98.38%: 100%|██████████| 196/196 [00:53<00:00,  3.64it/s]\n",
            "Test Epoch: [48/50] Loss: 0.4448 ACC@1: 84.54% ACC@5: 99.53%: 100%|██████████| 40/40 [00:05<00:00,  7.27it/s]\n",
            "Train Epoch: [49/50] Loss: 0.6084 ACC@1: 78.79% ACC@5: 98.35%: 100%|██████████| 196/196 [00:52<00:00,  3.70it/s]\n",
            "Test Epoch: [49/50] Loss: 0.4405 ACC@1: 84.69% ACC@5: 99.51%: 100%|██████████| 40/40 [00:05<00:00,  7.37it/s]\n",
            "Train Epoch: [50/50] Loss: 0.6133 ACC@1: 78.63% ACC@5: 98.36%: 100%|██████████| 196/196 [00:52<00:00,  3.72it/s]\n",
            "Test Epoch: [50/50] Loss: 0.4305 ACC@1: 85.15% ACC@5: 99.54%: 100%|██████████| 40/40 [00:05<00:00,  7.38it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}