{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoojungsun0/Psych239/blob/master/Linear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTqWQ1GTSqeY",
        "outputId": "9db2b0e7-b1da-40d5-c47b-5a76aa3e7d0e"
      },
      "source": [
        "!pip install thop"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: thop in /usr/local/lib/python3.6/dist-packages (0.0.31.post2005241907)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from thop) (1.7.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2jZOoMMQ2NA",
        "outputId": "6b9e72e4-bfb3-436c-8390-5d105f7ef8ed"
      },
      "source": [
        "\n",
        "\n",
        "import argparse\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from thop import profile, clever_format\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "import sys\n",
        "%cd /content/gdrive/MyDrive/'Colab Notebooks'/SimCLR\n",
        "\n",
        "\n",
        "import utils\n",
        "from model import Model"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "/content/gdrive/MyDrive/Colab Notebooks/SimCLR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXpV3lvBQ75r"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_class, pretrained_path):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # encoder\n",
        "        self.f = Model().f\n",
        "        # classifier\n",
        "        self.fc = nn.Linear(2048, num_class, bias=True)\n",
        "        self.load_state_dict(torch.load(pretrained_path, map_location='cpu'), strict=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(x)\n",
        "        feature = torch.flatten(x, start_dim=1)\n",
        "        out = self.fc(feature)\n",
        "        return out\n",
        "\n",
        "\n",
        "# train or test for one epoch\n",
        "def train_val(net, data_loader, train_optimizer):\n",
        "    is_train = train_optimizer is not None\n",
        "    net.train() if is_train else net.eval()\n",
        "\n",
        "    total_loss, total_correct_1, total_correct_5, total_num, data_bar = 0.0, 0.0, 0.0, 0, tqdm(data_loader)\n",
        "    with (torch.enable_grad() if is_train else torch.no_grad()):\n",
        "        for data, target in data_bar:\n",
        "            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
        "            out = net(data)\n",
        "            loss = loss_criterion(out, target)\n",
        "\n",
        "            if is_train:\n",
        "                train_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                train_optimizer.step()\n",
        "\n",
        "            total_num += data.size(0)\n",
        "            total_loss += loss.item() * data.size(0)\n",
        "            prediction = torch.argsort(out, dim=-1, descending=True)\n",
        "            total_correct_1 += torch.sum((prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
        "            total_correct_5 += torch.sum((prediction[:, 0:5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
        "\n",
        "            data_bar.set_description('{} Epoch: [{}/{}] Loss: {:.4f} ACC@1: {:.2f}% ACC@5: {:.2f}%'\n",
        "                                     .format('Train' if is_train else 'Test', epoch, epochs, total_loss / total_num,\n",
        "                                             total_correct_1 / total_num * 100, total_correct_5 / total_num * 100))\n",
        "\n",
        "    return total_loss / total_num, total_correct_1 / total_num * 100, total_correct_5 / total_num * 100"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2WPPuCZQ8EC",
        "outputId": "486fd07d-766e-47b0-f3ea-b5dc2b2c5a5c"
      },
      "source": [
        "# import os\n",
        "# rootdir = '../'\n",
        "model_path = 'results/Model2_onevec/128_0.5_200_256_100_model.pth'\n",
        "# model_path = os.path.join(rootdir, path)\n",
        "#'results/128_0.5_200_256_100_model.pth'\n",
        "batch_size = 256 #512\n",
        "epochs = 50 #100\n",
        "\n",
        "train_data = CIFAR10(root='data', train=True, transform=utils.train_transform, download=True)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
        "test_data = CIFAR10(root='data', train=False, transform=utils.test_transform, download=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
        "\n",
        "model = Net(num_class=len(train_data.classes), pretrained_path=model_path).cuda()\n",
        "for param in model.f.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "flops, params = profile(model, inputs=(torch.randn(1, 3, 32, 32).cuda(),))\n",
        "flops, params = clever_format([flops, params])\n",
        "print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3, weight_decay=1e-6)\n",
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "results = {'train_loss': [], 'train_acc@1': [], 'train_acc@5': [],\n",
        "            'test_loss': [], 'test_acc@1': [], 'test_acc@5': []}\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss, train_acc_1, train_acc_5 = train_val(model, train_loader, optimizer)\n",
        "    results['train_loss'].append(train_loss)\n",
        "    results['train_acc@1'].append(train_acc_1)\n",
        "    results['train_acc@5'].append(train_acc_5)\n",
        "    test_loss, test_acc_1, test_acc_5 = train_val(model, test_loader, None)\n",
        "    results['test_loss'].append(test_loss)\n",
        "    results['test_acc@1'].append(test_acc_1)\n",
        "    results['test_acc@5'].append(test_acc_5)\n",
        "    # save statistics\n",
        "    data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))\n",
        "    data_frame.to_csv('results/Model2_onevec/linear_statistics.csv', index_label='epoch')\n",
        "    if test_acc_1 > best_acc:\n",
        "        best_acc = test_acc_1\n",
        "        torch.save(model.state_dict(), 'results/Model2_onevec/linear_model.pth')\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/196 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torchvision.models.resnet.Bottleneck'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.Net'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "# Model Params: 23.52M FLOPs: 1.30G\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: [1/50] Loss: 0.8938 ACC@1: 69.63% ACC@5: 96.76%: 100%|██████████| 196/196 [00:53<00:00,  3.63it/s]\n",
            "Test Epoch: [1/50] Loss: 0.6236 ACC@1: 78.09% ACC@5: 98.91%: 100%|██████████| 40/40 [00:05<00:00,  7.11it/s]\n",
            "Train Epoch: [2/50] Loss: 0.7656 ACC@1: 73.05% ACC@5: 97.76%: 100%|██████████| 196/196 [00:54<00:00,  3.57it/s]\n",
            "Test Epoch: [2/50] Loss: 0.6047 ACC@1: 78.88% ACC@5: 98.86%: 100%|██████████| 40/40 [00:05<00:00,  7.25it/s]\n",
            "Train Epoch: [3/50] Loss: 0.7428 ACC@1: 74.00% ACC@5: 97.81%: 100%|██████████| 196/196 [00:54<00:00,  3.59it/s]\n",
            "Test Epoch: [3/50] Loss: 0.5767 ACC@1: 79.61% ACC@5: 99.19%: 100%|██████████| 40/40 [00:05<00:00,  7.34it/s]\n",
            "Train Epoch: [4/50] Loss: 0.7327 ACC@1: 74.32% ACC@5: 97.79%: 100%|██████████| 196/196 [00:54<00:00,  3.59it/s]\n",
            "Test Epoch: [4/50] Loss: 0.5509 ACC@1: 80.71% ACC@5: 99.16%: 100%|██████████| 40/40 [00:05<00:00,  7.28it/s]\n",
            "Train Epoch: [5/50] Loss: 0.7189 ACC@1: 74.77% ACC@5: 97.97%: 100%|██████████| 196/196 [00:53<00:00,  3.68it/s]\n",
            "Test Epoch: [5/50] Loss: 0.5489 ACC@1: 80.94% ACC@5: 99.19%: 100%|██████████| 40/40 [00:05<00:00,  7.37it/s]\n",
            "Train Epoch: [6/50] Loss: 0.7148 ACC@1: 74.96% ACC@5: 97.95%: 100%|██████████| 196/196 [00:53<00:00,  3.69it/s]\n",
            "Test Epoch: [6/50] Loss: 0.5370 ACC@1: 81.22% ACC@5: 99.27%: 100%|██████████| 40/40 [00:05<00:00,  7.35it/s]\n",
            "Train Epoch: [7/50] Loss: 0.7139 ACC@1: 74.94% ACC@5: 97.96%: 100%|██████████| 196/196 [00:52<00:00,  3.74it/s]\n",
            "Test Epoch: [7/50] Loss: 0.5439 ACC@1: 80.81% ACC@5: 99.06%: 100%|██████████| 40/40 [00:05<00:00,  7.35it/s]\n",
            "Train Epoch: [8/50] Loss: 0.7074 ACC@1: 75.16% ACC@5: 98.02%: 100%|██████████| 196/196 [00:52<00:00,  3.76it/s]\n",
            "Test Epoch: [8/50] Loss: 0.5356 ACC@1: 81.13% ACC@5: 99.26%: 100%|██████████| 40/40 [00:05<00:00,  7.32it/s]\n",
            "Train Epoch: [9/50] Loss: 0.7036 ACC@1: 75.40% ACC@5: 97.95%: 100%|██████████| 196/196 [00:51<00:00,  3.79it/s]\n",
            "Test Epoch: [9/50] Loss: 0.5477 ACC@1: 80.86% ACC@5: 99.12%: 100%|██████████| 40/40 [00:05<00:00,  7.48it/s]\n",
            "Train Epoch: [10/50] Loss: 0.7016 ACC@1: 75.45% ACC@5: 98.01%: 100%|██████████| 196/196 [00:51<00:00,  3.83it/s]\n",
            "Test Epoch: [10/50] Loss: 0.5267 ACC@1: 81.50% ACC@5: 99.23%: 100%|██████████| 40/40 [00:05<00:00,  7.52it/s]\n",
            "Train Epoch: [11/50] Loss: 0.6919 ACC@1: 75.70% ACC@5: 98.15%: 100%|██████████| 196/196 [00:52<00:00,  3.72it/s]\n",
            "Test Epoch: [11/50] Loss: 0.5394 ACC@1: 80.83% ACC@5: 99.17%: 100%|██████████| 40/40 [00:05<00:00,  7.43it/s]\n",
            "Train Epoch: [12/50] Loss: 0.6954 ACC@1: 75.65% ACC@5: 98.02%: 100%|██████████| 196/196 [00:52<00:00,  3.74it/s]\n",
            "Test Epoch: [12/50] Loss: 0.5238 ACC@1: 81.45% ACC@5: 99.18%: 100%|██████████| 40/40 [00:05<00:00,  7.42it/s]\n",
            "Train Epoch: [13/50] Loss: 0.6970 ACC@1: 75.62% ACC@5: 98.00%: 100%|██████████| 196/196 [00:52<00:00,  3.71it/s]\n",
            "Test Epoch: [13/50] Loss: 0.5159 ACC@1: 81.79% ACC@5: 99.22%: 100%|██████████| 40/40 [00:05<00:00,  7.45it/s]\n",
            "Train Epoch: [14/50] Loss: 0.6872 ACC@1: 75.79% ACC@5: 98.18%: 100%|██████████| 196/196 [00:53<00:00,  3.65it/s]\n",
            "Test Epoch: [14/50] Loss: 0.5194 ACC@1: 81.76% ACC@5: 99.20%: 100%|██████████| 40/40 [00:05<00:00,  7.42it/s]\n",
            "Train Epoch: [15/50] Loss: 0.6918 ACC@1: 75.87% ACC@5: 98.05%: 100%|██████████| 196/196 [00:52<00:00,  3.76it/s]\n",
            "Test Epoch: [15/50] Loss: 0.5137 ACC@1: 82.36% ACC@5: 99.34%: 100%|██████████| 40/40 [00:05<00:00,  7.39it/s]\n",
            "Train Epoch: [16/50] Loss: 0.6870 ACC@1: 75.91% ACC@5: 98.15%: 100%|██████████| 196/196 [00:53<00:00,  3.67it/s]\n",
            "Test Epoch: [16/50] Loss: 0.5258 ACC@1: 81.51% ACC@5: 99.15%: 100%|██████████| 40/40 [00:05<00:00,  7.32it/s]\n",
            "Train Epoch: [17/50] Loss: 0.6874 ACC@1: 75.92% ACC@5: 98.00%: 100%|██████████| 196/196 [00:53<00:00,  3.65it/s]\n",
            "Test Epoch: [17/50] Loss: 0.5107 ACC@1: 82.18% ACC@5: 99.33%: 100%|██████████| 40/40 [00:05<00:00,  7.32it/s]\n",
            "Train Epoch: [18/50] Loss: 0.6788 ACC@1: 76.16% ACC@5: 98.11%: 100%|██████████| 196/196 [00:52<00:00,  3.70it/s]\n",
            "Test Epoch: [18/50] Loss: 0.5091 ACC@1: 82.24% ACC@5: 99.22%: 100%|██████████| 40/40 [00:05<00:00,  7.31it/s]\n",
            "Train Epoch: [19/50] Loss: 0.6762 ACC@1: 76.26% ACC@5: 98.11%: 100%|██████████| 196/196 [00:52<00:00,  3.72it/s]\n",
            "Test Epoch: [19/50] Loss: 0.5013 ACC@1: 82.54% ACC@5: 99.27%: 100%|██████████| 40/40 [00:05<00:00,  7.42it/s]\n",
            "Train Epoch: [20/50] Loss: 0.6805 ACC@1: 76.29% ACC@5: 98.21%: 100%|██████████| 196/196 [00:52<00:00,  3.73it/s]\n",
            "Test Epoch: [20/50] Loss: 0.5060 ACC@1: 82.48% ACC@5: 99.24%: 100%|██████████| 40/40 [00:05<00:00,  7.31it/s]\n",
            "Train Epoch: [21/50] Loss: 0.6809 ACC@1: 76.15% ACC@5: 98.08%: 100%|██████████| 196/196 [00:51<00:00,  3.78it/s]\n",
            "Test Epoch: [21/50] Loss: 0.5007 ACC@1: 82.76% ACC@5: 99.34%: 100%|██████████| 40/40 [00:05<00:00,  7.35it/s]\n",
            "Train Epoch: [22/50] Loss: 0.6780 ACC@1: 76.22% ACC@5: 98.13%: 100%|██████████| 196/196 [00:54<00:00,  3.57it/s]\n",
            "Test Epoch: [22/50] Loss: 0.5002 ACC@1: 82.53% ACC@5: 99.36%: 100%|██████████| 40/40 [00:05<00:00,  7.52it/s]\n",
            "Train Epoch: [23/50] Loss: 0.6763 ACC@1: 76.42% ACC@5: 98.07%: 100%|██████████| 196/196 [00:50<00:00,  3.85it/s]\n",
            "Test Epoch: [23/50] Loss: 0.4926 ACC@1: 82.92% ACC@5: 99.41%: 100%|██████████| 40/40 [00:05<00:00,  7.55it/s]\n",
            "Train Epoch: [24/50] Loss: 0.6752 ACC@1: 76.36% ACC@5: 98.13%: 100%|██████████| 196/196 [00:51<00:00,  3.82it/s]\n",
            "Test Epoch: [24/50] Loss: 0.5029 ACC@1: 82.37% ACC@5: 99.31%: 100%|██████████| 40/40 [00:05<00:00,  7.28it/s]\n",
            "Train Epoch: [25/50] Loss: 0.6725 ACC@1: 76.35% ACC@5: 98.24%: 100%|██████████| 196/196 [00:54<00:00,  3.59it/s]\n",
            "Test Epoch: [25/50] Loss: 0.5292 ACC@1: 81.29% ACC@5: 99.21%: 100%|██████████| 40/40 [00:05<00:00,  7.52it/s]\n",
            "Train Epoch: [26/50] Loss: 0.6753 ACC@1: 76.27% ACC@5: 98.13%: 100%|██████████| 196/196 [00:51<00:00,  3.83it/s]\n",
            "Test Epoch: [26/50] Loss: 0.4959 ACC@1: 82.84% ACC@5: 99.35%: 100%|██████████| 40/40 [00:05<00:00,  7.48it/s]\n",
            "Train Epoch: [27/50] Loss: 0.6743 ACC@1: 76.52% ACC@5: 98.12%: 100%|██████████| 196/196 [00:50<00:00,  3.86it/s]\n",
            "Test Epoch: [27/50] Loss: 0.5027 ACC@1: 82.28% ACC@5: 99.36%: 100%|██████████| 40/40 [00:05<00:00,  7.42it/s]\n",
            "Train Epoch: [28/50] Loss: 0.6700 ACC@1: 76.46% ACC@5: 98.16%: 100%|██████████| 196/196 [00:50<00:00,  3.89it/s]\n",
            "Test Epoch: [28/50] Loss: 0.4924 ACC@1: 83.10% ACC@5: 99.20%: 100%|██████████| 40/40 [00:05<00:00,  7.49it/s]\n",
            "Train Epoch: [29/50] Loss: 0.6750 ACC@1: 76.33% ACC@5: 98.15%: 100%|██████████| 196/196 [00:50<00:00,  3.87it/s]\n",
            "Test Epoch: [29/50] Loss: 0.4906 ACC@1: 83.01% ACC@5: 99.26%: 100%|██████████| 40/40 [00:05<00:00,  7.54it/s]\n",
            "Train Epoch: [30/50] Loss: 0.6707 ACC@1: 76.51% ACC@5: 98.24%: 100%|██████████| 196/196 [00:50<00:00,  3.86it/s]\n",
            "Test Epoch: [30/50] Loss: 0.4911 ACC@1: 82.98% ACC@5: 99.27%: 100%|██████████| 40/40 [00:05<00:00,  7.47it/s]\n",
            "Train Epoch: [31/50] Loss: 0.6646 ACC@1: 76.79% ACC@5: 98.11%: 100%|██████████| 196/196 [00:50<00:00,  3.91it/s]\n",
            "Test Epoch: [31/50] Loss: 0.4974 ACC@1: 82.50% ACC@5: 99.28%: 100%|██████████| 40/40 [00:05<00:00,  7.52it/s]\n",
            "Train Epoch: [32/50] Loss: 0.6672 ACC@1: 76.84% ACC@5: 98.12%: 100%|██████████| 196/196 [00:50<00:00,  3.86it/s]\n",
            "Test Epoch: [32/50] Loss: 0.4833 ACC@1: 83.35% ACC@5: 99.33%: 100%|██████████| 40/40 [00:05<00:00,  7.32it/s]\n",
            "Train Epoch: [33/50] Loss: 0.6682 ACC@1: 76.50% ACC@5: 98.16%: 100%|██████████| 196/196 [00:51<00:00,  3.83it/s]\n",
            "Test Epoch: [33/50] Loss: 0.4974 ACC@1: 82.74% ACC@5: 99.32%: 100%|██████████| 40/40 [00:05<00:00,  7.46it/s]\n",
            "Train Epoch: [34/50] Loss: 0.6654 ACC@1: 76.66% ACC@5: 98.16%: 100%|██████████| 196/196 [00:50<00:00,  3.91it/s]\n",
            "Test Epoch: [34/50] Loss: 0.4733 ACC@1: 83.78% ACC@5: 99.39%: 100%|██████████| 40/40 [00:05<00:00,  7.51it/s]\n",
            "Train Epoch: [35/50] Loss: 0.6676 ACC@1: 76.38% ACC@5: 98.15%: 100%|██████████| 196/196 [00:50<00:00,  3.87it/s]\n",
            "Test Epoch: [35/50] Loss: 0.4797 ACC@1: 83.46% ACC@5: 99.30%: 100%|██████████| 40/40 [00:05<00:00,  7.30it/s]\n",
            "Train Epoch: [36/50] Loss: 0.6612 ACC@1: 76.96% ACC@5: 98.23%: 100%|██████████| 196/196 [00:51<00:00,  3.81it/s]\n",
            "Test Epoch: [36/50] Loss: 0.4813 ACC@1: 83.53% ACC@5: 99.28%: 100%|██████████| 40/40 [00:05<00:00,  7.48it/s]\n",
            "Train Epoch: [37/50] Loss: 0.6670 ACC@1: 76.46% ACC@5: 98.22%: 100%|██████████| 196/196 [00:50<00:00,  3.88it/s]\n",
            "Test Epoch: [37/50] Loss: 0.4858 ACC@1: 83.50% ACC@5: 99.29%: 100%|██████████| 40/40 [00:05<00:00,  7.46it/s]\n",
            "Train Epoch: [38/50] Loss: 0.6648 ACC@1: 76.60% ACC@5: 98.18%: 100%|██████████| 196/196 [00:50<00:00,  3.89it/s]\n",
            "Test Epoch: [38/50] Loss: 0.4898 ACC@1: 82.92% ACC@5: 99.19%: 100%|██████████| 40/40 [00:05<00:00,  7.56it/s]\n",
            "Train Epoch: [39/50] Loss: 0.6643 ACC@1: 76.58% ACC@5: 98.18%: 100%|██████████| 196/196 [00:50<00:00,  3.88it/s]\n",
            "Test Epoch: [39/50] Loss: 0.4823 ACC@1: 83.41% ACC@5: 99.30%: 100%|██████████| 40/40 [00:05<00:00,  7.48it/s]\n",
            "Train Epoch: [40/50] Loss: 0.6625 ACC@1: 76.69% ACC@5: 98.17%: 100%|██████████| 196/196 [00:50<00:00,  3.88it/s]\n",
            "Test Epoch: [40/50] Loss: 0.4903 ACC@1: 82.70% ACC@5: 99.37%: 100%|██████████| 40/40 [00:05<00:00,  7.44it/s]\n",
            "Train Epoch: [41/50] Loss: 0.6680 ACC@1: 76.53% ACC@5: 98.18%: 100%|██████████| 196/196 [00:50<00:00,  3.87it/s]\n",
            "Test Epoch: [41/50] Loss: 0.4881 ACC@1: 83.01% ACC@5: 99.37%: 100%|██████████| 40/40 [00:05<00:00,  7.44it/s]\n",
            "Train Epoch: [42/50] Loss: 0.6654 ACC@1: 76.57% ACC@5: 98.14%: 100%|██████████| 196/196 [00:50<00:00,  3.90it/s]\n",
            "Test Epoch: [42/50] Loss: 0.4749 ACC@1: 83.57% ACC@5: 99.37%: 100%|██████████| 40/40 [00:05<00:00,  7.45it/s]\n",
            "Train Epoch: [43/50] Loss: 0.6652 ACC@1: 76.78% ACC@5: 98.20%: 100%|██████████| 196/196 [00:50<00:00,  3.91it/s]\n",
            "Test Epoch: [43/50] Loss: 0.4837 ACC@1: 83.39% ACC@5: 99.30%: 100%|██████████| 40/40 [00:05<00:00,  7.59it/s]\n",
            "Train Epoch: [44/50] Loss: 0.6592 ACC@1: 77.04% ACC@5: 98.15%: 100%|██████████| 196/196 [00:50<00:00,  3.91it/s]\n",
            "Test Epoch: [44/50] Loss: 0.4817 ACC@1: 83.35% ACC@5: 99.38%: 100%|██████████| 40/40 [00:05<00:00,  7.44it/s]\n",
            "Train Epoch: [45/50] Loss: 0.6595 ACC@1: 77.05% ACC@5: 98.22%: 100%|██████████| 196/196 [00:50<00:00,  3.86it/s]\n",
            "Test Epoch: [45/50] Loss: 0.4685 ACC@1: 83.80% ACC@5: 99.40%: 100%|██████████| 40/40 [00:05<00:00,  7.60it/s]\n",
            "Train Epoch: [46/50] Loss: 0.6608 ACC@1: 76.71% ACC@5: 98.18%: 100%|██████████| 196/196 [00:51<00:00,  3.82it/s]\n",
            "Test Epoch: [46/50] Loss: 0.4701 ACC@1: 83.75% ACC@5: 99.43%: 100%|██████████| 40/40 [00:05<00:00,  7.42it/s]\n",
            "Train Epoch: [47/50] Loss: 0.6600 ACC@1: 76.73% ACC@5: 98.19%: 100%|██████████| 196/196 [00:50<00:00,  3.86it/s]\n",
            "Test Epoch: [47/50] Loss: 0.4792 ACC@1: 83.43% ACC@5: 99.43%: 100%|██████████| 40/40 [00:05<00:00,  7.43it/s]\n",
            "Train Epoch: [48/50] Loss: 0.6603 ACC@1: 76.87% ACC@5: 98.19%: 100%|██████████| 196/196 [00:50<00:00,  3.87it/s]\n",
            "Test Epoch: [48/50] Loss: 0.4864 ACC@1: 82.94% ACC@5: 99.34%: 100%|██████████| 40/40 [00:05<00:00,  7.29it/s]\n",
            "Train Epoch: [49/50] Loss: 0.6563 ACC@1: 76.94% ACC@5: 98.19%: 100%|██████████| 196/196 [00:50<00:00,  3.87it/s]\n",
            "Test Epoch: [49/50] Loss: 0.4737 ACC@1: 83.56% ACC@5: 99.44%: 100%|██████████| 40/40 [00:05<00:00,  7.53it/s]\n",
            "Train Epoch: [50/50] Loss: 0.6541 ACC@1: 77.14% ACC@5: 98.22%: 100%|██████████| 196/196 [00:50<00:00,  3.90it/s]\n",
            "Test Epoch: [50/50] Loss: 0.4865 ACC@1: 83.04% ACC@5: 99.36%: 100%|██████████| 40/40 [00:05<00:00,  7.55it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}